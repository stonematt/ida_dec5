{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy Unit 4, \n",
    "## Day 1-ish\n",
    "### Hypothesis Tests\n",
    "When observing differences in data, a data analyst understands the possibility that these differences could be the result of random chance.\n",
    "\n",
    "Suppose we want to know if men are more likely to sign up for a given programming class than women. We invite 100 men and 100 women to this class. After one week, 34 women sign up, and 39 men sign up. More men than women signed up, but is this a \"real\" difference?\n",
    "\n",
    "We have taken sample means from two different populations, men and women. We want to know if the difference that we observe in these sample means reflects a difference in the population means. To formally answer this question, we need to re-frame it in terms of probability:\n",
    "\n",
    "\"What is the probability that men and women have the same level of interest in this class and that the difference we observed is just chance?\"\n",
    "\n",
    "In other words, \"If we gave the same invitation to every person in the world, would more men still sign up?\"\n",
    "\n",
    "A more formal version is: \"What is the probability that the two population means are the same and that the difference we observed in the sample means is just chance?\"\n",
    "\n",
    "These statements are all ways of expressing a *null hypothesis*. `A null hypothesis is a statement that the observed difference is the result of chance.`\n",
    "\n",
    "Hypothesis testing is a mathematical way of determining whether we can be confident that the null hypothesis is false. Different situations will require different types of hypothesis testing, which we will learn about in the next lesson.\n",
    "\n",
    "### Type I Or Type II\n",
    "When we rely on automated processes to make our decisions for us, we need to be aware of how this automation can lead to mistakes. Computer programs are as fallible as the humans who design them. As humans capable of programming, the responsibility is on us to understand what can go wrong and what we can do to contain these foreseeable problems.\n",
    "\n",
    "In statistical hypothesis testing, we concern ourselves primarily with two types of error. The first kind of error, known as a `Type I error, is finding a correlation between things that are not related.` This error is sometimes called a \"false positive\" and occurs when the null hypothesis is rejected even though it is true.\n",
    "\n",
    "For example, let's say you conduct an A/B test for an online store and conclude that interface B is significantly better than interface A at directing traffic to a checkout page. You have rejected the null hypothesis that there is no difference between the two interfaces. If, in reality, your results were due to the groups you happened to pick, and there is actually no significant difference between interface A and interface B in the greater population, you have been the victim of a false positive.\n",
    "\n",
    "The second kind of error, a `Type II error, is failing to find a correlation between things that are actually related.` This error is referred to as a \"false negative\" and occurs when the null hypothesis is accepted even though it is false.\n",
    "\n",
    "For example, with the A/B test situation, let's say that after the test, you concluded that there was no significant difference between interface A and interface B. If there actually is a difference in the population as a whole, your test has resulted in a false negative.\n",
    "\n",
    "### P-Values\n",
    "We have discussed how a hypothesis test is used to determine the validity of a null hypothesis. A hypothesis test provides a numerical answer, called a p-value, that helps us decide how confident we can be in the result. A p-value is the probability that the null hypothesis is true.\n",
    "\n",
    "`A p-value of 0.05 would mean that there is a 5% chance that the null hypothesis is true.` This generally means there is a 5% chance that there is no difference between the two population means.\n",
    "\n",
    "Before conducting a hypothesis test, we determine the necessary threshold we would need before concluding that the results are significant. A higher p-value is more likely to give a false positive so if we want to be very sure that the result is not due to just chance, we will select a very small p-value.\n",
    "\n",
    "It is important that we choose the p-value before we see the results. If we wait until after we see the results, we might pick our threshold such that we get the result we want to see. For instance, if we're trying to publish our results, we might set a p-value such that our results are significant. Choosing our p-value in advance helps keep us honest.\n",
    "\n",
    "`Generally, we want a p-value of less than 0.05, meaning that there is a less than 5% chance that our results are due to random chance.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4-ish \n",
    "### 1 Sample T-Test\n",
    "\n",
    "We can test this using a univariate T-test. A univariate T-test `compares a sample mean to a hypothetical population mean`. It answers the question \"What is the probability that the sample came from a distribution with the desired mean?\"\n",
    "\n",
    "When we conduct a hypothesis test, we want to first create a null hypothesis, which is a prediction that there is no significant difference. The null hypothesis that this test examines can be phrased as such: \"The set of samples belongs to a population with the target mean\".\n",
    "\n",
    "The result of the 1 Sample T Test is a p-value, which will tell us whether or not we can reject this null hypothesis. \n",
    "\n",
    "> Generally, if we receive a p-value of less than 0.05, we can reject the null hypothesis and state that there is a significant difference.\n",
    "\n",
    "```\n",
    "from scipy.stats import ttest_1samp\n",
    "tstat, pval = ttest_1samp(example_distribution, expected_mean)\n",
    "print pval\n",
    "```\n",
    "\n",
    "### 2 Sample T-Test\n",
    "Suppose that last week, the average amount of time spent per visitor to a website was 25 minutes. This week, the average amount of time spent per visitor to a website was 28 minutes. Did the average time spent per visitor change? Or is this part of natural fluctuations?\n",
    "\n",
    "One way of testing whether this difference is significant is by using a 2 Sample T-Test. `A 2 Sample T-Test compares two sets of data, which are both approximately normally distributed.`\n",
    "\n",
    "*The null hypothesis, in this case, is that the two distributions have the same mean.*\n",
    "\n",
    "We can use SciPy's `ttest_ind` function to perform a 2 Sample T-Test. It takes the two distributions as inputs and returns the t-statistic (which we don't use), and a p-value. If you can't remember what a p-value is, refer to the earlier exercise on univariate t-tests.\n",
    "\n",
    "```\n",
    "from scipy.stats import ttest_ind\n",
    "tstat, pval = ttest_ind(sample1, sample2)\n",
    "print pval\n",
    "```\n",
    "\n",
    "#### Dangers of Multiple T-Tests\n",
    "Suppose that we own a chain of stores that sell ants, called VeryAnts. There are three different locations: A, B, and C. We want to know if the average ant sales over the past year are significantly different between the three locations.\n",
    "\n",
    "At first, it seems that we could perform T-tests between each pair of stores.\n",
    "\n",
    "We know that the p-value is the probability that we incorrectly reject the null hypothesis on each t-test. The more t-tests we perform, the more likely that we are to get a false positive, a Type I error.\n",
    "\n",
    "For a p-value of 0.05, if the null hypothesis is true then the probability of obtaining a significant result is 1 â€“ 0.05 = 0.95. When we run another t-test, the probability of still getting a correct result is 0.95 * 0.95, or 0.9025. That means our probability of making an error is now close to 10%! This error probability only gets bigger with the more t-tests we do.\n",
    "\n",
    "### ANOVA\n",
    "* More than 2 data sets\n",
    "* should follow a normal distribution\n",
    "* H(0): They're all the same, so a low p-value indicates 1 of the datasets is different, but doesn't indicate which (good first triage)\n",
    "\n",
    "In the last exercise, we saw that the probability of making a Type I error got dangerously high as we performed more t-tests.\n",
    "\n",
    "When comparing more than two numerical datasets, the best way to preserve a Type I error probability of 0.05 is to use ANOVA. ANOVA (Analysis of Variance) tests the null hypothesis that all of the datasets have the same mean. If we reject the null hypothesis with ANOVA, we're saying that at least one of the sets has a different mean; however, it does not tell us which datasets are different.\n",
    "\n",
    "We can use the SciPy function f_oneway to perform ANOVA on multiple datasets. It takes in each dataset as a different input and returns the t-statistic and the p-value. For example, if we were comparing scores on a videogame between math majors, writing majors, and psychology majors, we could run an ANOVA test with this line:\n",
    "```\n",
    "from scipy.stats import f_oneway\n",
    "tstat, pval = f_oneway(scores_mathematicians, scores_writers, scores_psychologists)\n",
    "```\n",
    "The null hypothesis, in this case, is that all three populations have the same mean score on this videogame. If we reject this null hypothesis (if we get a p-value less than 0.05), we can say that we are reasonably confident that a pair of datasets is significantly different. After using only ANOVA, we can't make any conclusions on which two populations have a significant difference.\n",
    "## Dangers of these tests (look out for):\n",
    "\n",
    "* THE SAMPLES SHOULD EACH BE NORMALLY DISTRIBUTED...ISH\n",
    "* THE POPULATION STANDARD DEVIATIONS OF THE GROUPS SHOULD BE EQUAL\n",
    ">For ANOVA and 2-Sample T-Tests, using datasets with standard deviations that are significantly different from each other will often obscure the differences in group means.\n",
    ">\n",
    ">To check for similarity between the standard deviations, it is normally sufficient to divide the two standard deviations and see if the ratio is \"close enough\" to 1. \"Close enough\" may differ in different contexts but generally staying within 10% should suffice.\n",
    "* THE SAMPLES MUST BE INDEPENDENT\n",
    ">When comparing two or more datasets, the values in one distribution should not affect the values in another distribution. In other words, knowing more about one distribution should not give you any information about any other distribution.\n",
    "\n",
    "### Tukey's Range Test\n",
    "Let's say that we have performed ANOVA to compare three sets of data from the three VeryAnts stores. We received the result that there is some significant difference between datasets.\n",
    "\n",
    "Now, we have to find out which datasets are different.\n",
    "\n",
    "We can perform a Tukey's Range Test to determine the difference between datasets.\n",
    "\n",
    "If we feed in three datasets, such as the sales at the VeryAnts store locations A, B, and C, Tukey's Test can tell us which pairs of locations are distinguishable from each other.\n",
    "\n",
    "The function to perform Tukey's Range Test is `pairwise_tukeyhsd`, which is found in `statsmodel`, not scipy. We have to provide the function with one list of all of the data and a list of labels that tell the function which elements of the list are from which set. We also provide the significance level we want, which is usually 0.05.\n",
    "\n",
    "For example, if we were looking to compare mean scores of movies that are dramas, comedies, or documentaries, we would make a call to pairwise_tukeyhsd like this:\n",
    "```\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "movie_scores = np.concatenate([drama_scores, comedy_scores, documentary_scores])\n",
    "labels = ['drama'] * len(drama_scores) + ['comedy'] * len(comedy_scores) + ['documentary'] * len(documentary_scores)\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(movie_scores, labels, 0.05)\n",
    "```\n",
    "It will return a table of information, telling you whether or not to reject the null hypothesis for each pair of datasets.\n",
    "\n",
    "### Binomial Tests\n",
    "If we have a dataset where the entries are not numbers, but categories instead, we have to use different methods.\n",
    "\n",
    "To analyze a dataset like this, with two different possibilities for entries, we can use a Binomial Test. A Binomial Test compares a categorical dataset to some expectation.\n",
    "\n",
    "Examples include:\n",
    "* Comparing the actual percent of emails that were opened to the quarterly goals\n",
    "* Comparing the actual percentage of respondents who gave a certain survey response to the expected survey response\n",
    "* Comparing the actual number of heads from 1000 coin flips of a weighted coin to the expected number of heads\n",
    "The null hypothesis, in this case, would be that there is no difference between the observed behavior and the expected behavior. If we get a p-value of more than 0.05, we can reject that hypothesis and determine that there is a difference between the observation and expectation.\n",
    "\n",
    "SciPy has a function called `binom_test`, which performs a Binomial Test for you.\n",
    "\n",
    "`binom_test` requires three inputs, the number of observed successes, the number of total trials, and an expected probability of success. For example, with 1000 coin flips of a fair coin, we would expect a \"success rate\" (the rate of getting heads), to be 0.5, and the number of trials to be 1000. Let's imagine we get 525 heads. Is the coin weighted? This function call would look like:\n",
    "\n",
    "```\n",
    "from scipy.stats import binom_test\n",
    "pval = binom_test(525, n=1000, p=0.5)```\n",
    "\n",
    "It returns a p-value, telling us how confident we can be that the sample of values was likely to occur with the specified probability. If we get a p-value less than 0.05, we can reject the null hypothesis and say that it is likely the coin is actually weighted, and that the probability of getting heads is statistically different than 0.5\n",
    "\n",
    "### Chi Square Test\n",
    "In the last exercise, we looked at data where customers visited a website and either made a purchase or did not make a purchase. What if we also wanted to track if visitors added any items to their shopping cart? With three discrete categories of data per dataset, we can no longer use a Binomial Test. If we have two or more categorical datasets that we want to compare, we should use a Chi Square test. It is useful in situations like:\n",
    "\n",
    "* An A/B test where half of users were shown a green submit button and the other half were shown a purple submit button. Was one group more likely to click the submit button?\n",
    "* Men and women were both given a survey asking \"Which of the following three products is your favorite?\" Did the men and women have significantly different preferences?\n",
    "In SciPy, you can use the function `chi2_contingency` to perform a Chi Square test.\n",
    "\n",
    "The input to `chi2_contingency` is a contingency table where:\n",
    "\n",
    "* The columns are each a different condition, such as men vs. women or Interface A vs. Interface B\n",
    "* The rows represent different outcomes, like \"Survey Response A\" vs. \"Survey Response B\" or \"Clicked a Link\" vs. \"Didn't Click\"\n",
    "This table can have as many rows and columns as you need.\n",
    "\n",
    "In this case, the null hypothesis is that there's no significant difference between the datasets. We reject that hypothesis, and state that there is a significant difference between two of the datasets if we get a p-value less than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.155082308077 is greater than 0.05, so accept - not strong evidence of difference\n",
      "0.00281283455955 is lower that 0.05, so there is strong evidence of difference\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "'''\n",
    "The management at the VeryAnts ant store wants to know if \n",
    "their two most popular species of ants, the Leaf Cutter and \n",
    "the Harvester, vary in popularity between 1st, 2nd, and 3rd graders.\n",
    "\n",
    "We have created a table representing the different ants bought \n",
    "by the children in grades 1, 2, and 3 after the last big field \n",
    "trip to VeryAnts. Run the code to see what happens when we \n",
    "enter this table into SciPy's chi-square test.\n",
    "\n",
    "'''\n",
    "# Contingency table\n",
    "#         harvester |  leaf cutter\n",
    "# ----+------------------+------------\n",
    "# 1st gr | 30       |  10\n",
    "# 2nd gr | 35       |  5\n",
    "# 3rd gr | 28       |  12\n",
    "\n",
    "Y = [[30, 10],\n",
    "     [35, 5],\n",
    "     [28, 12]]\n",
    "chi2, pval_y, dof, expected = chi2_contingency(Y)\n",
    "\n",
    "'''\n",
    "Does the resulting p-value mean that we should reject or \n",
    "accept the null hypothesis?'''\n",
    "print(pval_y, \"is greater than 0.05, so accept - not strong evidence of difference\")\n",
    "\n",
    "'''A class of 40 4th graders comes into VeryAnts in the next week and buys 20 sets \n",
    "of Leaf Cutter ants and 20 sets of Harvester ants.\n",
    "\n",
    "Add this data to the contingency table, rerun the chi-square test, and see if \n",
    "there is now a low enough value to reject the null hypothesis.'''\n",
    "\n",
    "X = [[30, 10],\n",
    "     [35, 5],\n",
    "     [28, 12],\n",
    "     [20, 20]]\n",
    "chi2, pval_x, dof, expected = chi2_contingency(X)\n",
    "print(pval_x,\"is lower that 0.05, so there is strong evidence of difference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### going back to the election results project\n",
    "In A survey of 70 individuals, 47% chose Ceballos.  In the election, Ceballos earned 54% of th vote and won the election.\n",
    "\n",
    "Using a binomial distibution form numpy, we predicted Ceballos would lose in ~64% of the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survey suggests Ceballos had 64.13% chance of losing\n",
      "or a 35.87% chance of winning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "possible_surveys = np.random.binomial(70, \\\n",
    "                                      0.471428571429, \\\n",
    "                                      size=10000) / 70\n",
    "# how likely did the survey predict that\n",
    "# Ceballos would loose, (i.e. get < 50%)\n",
    "ceballos_loss_surveys = np.mean(possible_surveys < 0.5)\n",
    "print(\"survey suggests Ceballos had {0:.2f}% chance of losing\".format(ceballos_loss_surveys * 100))\n",
    "print(\"or a {0:.2f}% chance of winning\".format(100-(ceballos_loss_surveys * 100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what about the confidence in the survey?\n",
    "\n",
    "In scipy.stats we can do a binom test to understand if the observed outcome (54% of the vote) is due to random change based on the survey we did. \n",
    "* H(0): the difference between the sample 47% and real outcome of 54% is due to sampling, not survey design\n",
    "* H(a): the difference is real so so the suvey is likely flawed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.40149684593e-44\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binom_test\n",
    "# we observed 5400 of 10000 voters selected Ceballos, but we expected 47%\n",
    "pval = binom_test(5400, n=10000, p=0.47)\n",
    "print(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pval is very small ~1.4e-44, so we reject the null hypothesis.  This is not sampling error,  suvey wasn't a good indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @Jawells slack comment\n",
    "\n",
    "https://codecademyintensive.slack.com/archives/C88QH8YV6/p1515885315000001\n",
    "> I would say that you should use 0.57 as the probability for checking your survey. Here's why: we wanted to find out something about the population of people who were going to vote in the election (10000) by taking a sample, the 70 people who you surveyed. Your survey said 47% of people would vote for Ceballos, but since the election has happened you know that the true proportion was actually 54%. Since you know the true probability is 0.54, that is what you should compare to. So you should look at possible_surveys = np.random.binomial(70, .54,10000)/70.0 which says if I have a population of 10000 people, 54% of which favor Ceballos AND I sample 70 of those, what proportion of these samples will predict a loss for Ceballos. In my calculation it was about 21%. Then, when you survey 7000 of the 10000 people, your survey will never predict a loss for Ceballos, showing that increasing the sample size improves your prediction. This is how you know p should NOT be 0.47, because in that case it would predict a loss for Ceballos 100% of the time if you upped your sample size to 7000, which you know is wrong.\n",
    "\n",
    "`i think @jawells meant 0.54 above (not 0.57)`\n",
    "\n",
    "https://codecademyintensive.slack.com/archives/C88QH8YV6/p1515885648000047\n",
    "\n",
    "> It's also a good idea to step back and ask yourself what could explain your results. For instance, your survey could be wrong because instead of surveying only people who are eligible to vote in the Shelbyville election between Ceballos and Kerrigan. you accidentally surveyed people from Springfield, who were ineligible to vote here. In which case, you would need to change how you select your survey participants. But this analysis shows that the failure of your survey to predict the actual result could simply be due to chance. So unless you have some independent reason to believe that your sample included ineligible voters, the perhaps the best course of action for next election would be to simply increase your sample size, if you can afford it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowing the acutual results, that Ceballos got 54% of the vote, \n",
      "surveying 70 people of 10,000 would predict Ceballos losing 21.10% of the time\n",
      "or winning 78.90% of the time\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# in reality, we KNOW 57% actually voted for Ceballos in the election.\n",
    "# So, using 0.54, how likely was it that we would get a losing prediction(<50%) \n",
    "# in our suvey of 70 people?\n",
    "possible_surveys54 = np.random.binomial(70, \\\n",
    "                                      0.54, \\\n",
    "                                      size=10000) / 70\n",
    "# how likely would a sample size of 70 predict that\n",
    "# Ceballos would lose, (i.e. get < 50%)\n",
    "ceballos_loss_surveys54 = np.mean(possible_surveys57 < 0.5)\n",
    "print(\"Knowing the acutual results, that Ceballos got 54% of the vote, \")\n",
    "print(\"surveying 70 people of 10,000 would predict Ceballos losing {0:.2f}% of the time\".format(ceballos_loss_surveys54 * 100))\n",
    "print(\"or winning {0:.2f}% of the time\".format(100-(ceballos_loss_surveys54 * 100)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
